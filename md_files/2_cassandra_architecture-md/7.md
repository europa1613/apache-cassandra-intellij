Additional components of Cassandra
----------------------------------

* * * * *

Now that we have discussed the read and write paths of an individual
Apache Cassandra node, let's move up a level and consider how all of the
nodes work together. Keeping data consistent and serving requests in a
way that treats multiple machines as a single data source requires some
extra engineering. Here we'll explore the additional components which
make that possible.

### Gossiper

Gossiper is a peer-to-peer communication protocol that a node uses to
communicate with the other nodes in the cluster. When the nodes gossip
with each other, they share information about themselves and retrieve
information on a subset of other nodes in the cluster. Eventually, this
allows a node to store and understand state information about every
other node in the cluster.

Each node's gossiper runs once per second (Saha 2017) and initiates
communication with up to three other nodes. The gossip protocol first
runs when the node is attempting to join the cluster. As a new node
contains no information on the current network topology, it must be
preconfigured with a list of current live nodes or `seeds` to
help it start the communication process.

### Note

We will discuss the concept of seed nodes in lab 4, *Configuring a
Cluster*. The most important thing to remember about them, is that they
help new nodes discover the rest of the cluster's network topology when
joining. All nodes are considered to be peers in Cassandra, and the
seed-node designation does not change that.

You can view the currently perceived gossip state of any node using the
`nodetool` utility on any node. The following command
demonstrates what this looks like with a three-node cluster:

Copy

``` {.programlisting .language-markup}
bin/nodetool gossipinfo

/192.168.0.101
 generation:1524418542
 heartbeat:462
 STATUS:14:NORMAL,-9223372036854775808
 LOAD:205:161177.0
 SCHEMA:10:ea63e099-37c5-3d7b-9ace-32f4c833653d
 DC:6:ClockworkAngels
 RACK:8:R40
 RELEASE_VERSION:4:3.11.2
 RPC_ADDRESS:3:192.168.0.101
 NET_VERSION:1:11
 HOST_ID:2:0edb5efa-de6e-4512-9f5d-fe733c7d448c
 RPC_READY:20:true
 TOKENS:15:&lt;hidden&gt;
/192.168.0.102
 generation:1524418547
 heartbeat:249
 STATUS:14:NORMAL,-3074457345618258603
 LOAD:205:49013.0
 SCHEMA:10:ea63e099-37c5-3d7b-9ace-32f4c833653d
 DC:6:ClockworkAngels
 RACK:8:R40
 RELEASE_VERSION:4:3.11.2
 RPC_ADDRESS:3:192.168.0.102
 NET_VERSION:1:11
 HOST_ID:2:404a8f97-f5f0-451f-8b72-43575fc874fc
 RPC_READY:27:true
 TOKENS:15:&lt;hidden&gt;
/192.168.0.103
 generation:1524418548
 heartbeat:248
 STATUS:14:NORMAL,3074457345618258602
 LOAD:205:54039.0
 SCHEMA:10:ea63e099-37c5-3d7b-9ace-32f4c833653d
 DC:6:ClockworkAngels
 RACK:8:R40
 RELEASE_VERSION:4:3.11.2
 RPC_ADDRESS:3:192.168.0.103
 NET_VERSION:1:11
 HOST_ID:2:eed33fc5-98f0-4907-a02f-2a738e7af562
 RPC_READY:26:true
 TOKENS:15:&lt;hidden&gt;
```

As you can see, the gossip state of each node contains a wealth of
information about it. Things such as status, data load, host ID, and
schema version are readily visible. This is a good way to quickly gain
an understanding of one node's view of the cluster.

### Note

Remember, each node maintains its own view of the cluster's gossip
state. While it should be the same on all nodes, it sometimes is not. It
is entirely possible that some nodes can see certain nodes as down when
other nodes do not. The `nodetool gossipinfo` command can be a
valuable tool when troubleshooting firewall or network issues. 

### Snitch

Cassandra uses a component known as the snitch to direct read and write
operations to the appropriate nodes. When an operation is sent to the
cluster, it is the snitch's job (Williams 2012) to determine which nodes
in specific data centers or racks can serve the request.

Most snitches that ship with Apache Cassandra are both data center- and
rack-aware. That is, they are capable of viewing (predetermined) groups
of nodes in the cluster as **logical data centers**. Additionally, the
data centers can be further divided into logical racks.

### Note

The term **logical** is used here because the data centers may or may
not be physically separate from each other. That is, nodes in a single
physical data center can be split into smaller data centers, and the
snitch will enforce data-replication for each separately.

The replication of data per keyspace is defined in the keyspace
definition using the configuration properties of
`NetworkTopologyStrategy` . This can be seen in the following
example keyspace definition:

Copy

``` {.programlisting .language-markup}
CREATE KEYSPACE fenago WITH replication = {
   'class': 'NetworkTopologyStrategy',
   'ClockworkAngels': '3',
   'PermanentWaves': '2'
};
```

 

In the previous example, we'll assume the use of
`GossipingPropertyFileSnitch` , and that we have a number of
nodes in our cluster assigned to our `ClockworkAngels` data
center and some assigned to our `PermanentWaves` data center.
When a write operation occurs on any of the tables stored in the
`fenago` keyspace, the snitch will ensure that three replicas
of that data are written to the `ClockworkAngels` data center.
Likewise, two replicas of the data will be written to the
`PermanentWaves` data center.

As previously mentioned, the nodes in a data center can be further
divided into racks. Let's assume that our `ClockworkAngels` 
data center has six nodes. To maintain the maximum amount of
availability possible, we could divide them up like this:

Copy

``` {.programlisting .language-markup}
Data center: ClockworkAngels
Node             Rack
========================
192.168.255.1    East_1
192.168.255.2    West_1
192.168.255.3    West_1
192.168.255.4    East_1
192.168.255.5    West_1
192.168.255.6    East_1
```

### Note

Nodes in a data center are ordered by primary token range, not
necessarily by IP address. This example was ordered by IP for
simplicity.

Let's assume that our write has a partition key that hashes out to a
primary range owned by `192.168.255.2` , which is in the
`West_1` rack. The snitch will ensure that the first replica
is written to `192.168.255.2` . Since our keyspace has an RF of
three, the snitch will then walk the ring (clockwise by token ranges) to
place the remaining replicas. As there are multiple racks, it will try
to spread the replicas evenly across them. Therefore, it will place the
second replica on `192.168.255.3` , which is in a different
rack. As both racks are now covered from an availability perspective,
the third replica will be placed on the next node, regardless of rack
designation.

 

 

### Note

When working with physical data centers, entire racks have been known to
fail. Likewise, entire availability zones have been known to fail in the
cloud. Therefore, it makes sense to define logical racks with physical
racks or **Availability Zones** (**AZs**) whenever possible. This
configuration strategy provides the cluster with its best chance for
remaining available in the event of a rack/AZ outage. When defining your
cluster, data center, and rack topology, it makes sense to build a
number of nodes that evenly compliment your RF. For instance, with an RF
of three, the snitch will spread the data evenly over a total of three,
six, or nine nodes, split into three racks.

### Phi failure-detector

Apache Cassandra uses an implementation of the **Phi Failure-Accrual**
algorithm. The basic idea is that gossip between nodes is measured on a
scale that adjusts to current network conditions. The results are
compared to a preconfigured, static value
(`phi_convict_threshold` ) that helps a node decide whether
another node should be marked as down. `phi_convict_threshold` 
will be discussed in later labs.

### Tombstones

Deletes in the world of distributed databases are hard to do. After all,
how do you replicate nothing? Especially a key for nothing which once
held a value. Tombstones are Cassandra's way of solving that problem.

As Cassandra uses a log-based storage engine, deletes are essentially
writes. When data is deleted, a structure known as a **tombstone** is
written. This tombstone exists so that it can be replicated to the other
nodes that contained the deleted replica.

Discussion about tombstones tends to be more prevalent in Cassandra
application-developer circles. This is primarily because their
accumulation in large numbers can be problematic. Let's say that data is
written for a specific key, then deleted, and then written and deleted
three more times, before being written again. With the writes for that
key being stored sequentially, SSTable data files that back that key
will contain five possible values and four tombstones. Depending on how
often that write pattern continues, there could be thousands of
tombstoned and obsoleted stored across multiple files for that key. When
that key is queried, there is essentially an accumulation of garbage
that must be overlooked to find the actual, current value.

 

Accumulation of tombstones will result in slower queries over time. For
this reason, use cases that require a high amount of delete activity are
considered an anti-pattern with Apache Cassandra.

### Note

The Apache Cassandra configuration has settings for warnings and failure
thresholds regarding the number of tombstones that can be returned in a
query.

Tombstones are eventually removed, but only after two specific
conditions are met:

-   The tombstone has existed longer than the table's defined
    `gc_grace_seconds` period
-   Compaction runs on that table

The `gc_grace_seconds` stipulation exists to give the
tombstone ample time to either replicate, or be fixed by a repair
operation. After all, if tombstones are not adequately replicated, a
read to a node that is missing a tombstone can result in data ghosting
its way back into a result set.

### Hinted handoff

When a node reports another as down, the node that is still up will
store structures, known as **hints**, for the down node. Hints are
essentially writes meant for one node that are temporarily stored on
another. When the down node returns to a healthy status, the hints are
then streamed to that node in an attempt to re-sync its data.

### Note

As storage is finite, hints will be stored for up to three hours, by
default. This means that a down node must be brought back within that
time frame, or data loss will occur.

With Apache Cassandra versions older than 3.0, hints were stored in the
hints table of the system keyspace. The problem with this approach is
that once the stored hints were replayed, they were then deleted. This
frequently led to warnings and failures due to tombstone accumulation.
Apache Cassandra version 3.0 and up moved to a file-based system of hint
storage, thus avoiding this problem so long as the `hints` 
directory has disk space available.

 

### Compaction

As previously mentioned, once SSTable files are written to disk, they
are immutable (cannot be written to again). Additional writes for that
table would eventually result in additional SSTable files. As this
process continues, it is possible for long-standing rows of data to be
spread across multiple files. Reading multiple files to satisfy a query
eventually becomes slow, especially when considering how obsolete data
and tombstones must be reconciled (so as not to end up in the result
set).

Apache Cassandra's answer to this problem is to periodically execute a
process called **compaction**. When compaction runs, it performs the
following functions:

-   Multiple data files for a table are merged into a single data file
-   Obsoleted data is removed
-   Tombstones that have outlived the table's
    `gc_grace_seconds` period are removed

There are two different compaction strategies available with Apache
Cassandra: `SizeTieredCompactionStrategy` and
`LeveledCompactionStrategy` . These will be detailed in a later
lab. Compaction is configured on a per-table basis, and its
configuration includes thresholds for determining how often compaction
is run.

### Note

Cassandra does a good job of figuring out when compaction needs to be
run. The best approach is to let it do what it needs to do. If it should
be run sooner, then the configuration should be altered on the table in
question. Forcing compaction (via the command line) to run outside of
its configured thresholds has been known to drastically delay future
compaction times.

### Repair

Sometimes data replicas on one or more Cassandra nodes can get out of
sync. There are a variety of reasons for this, including prior network
instability, hardware failure, and nodes crashing and staying down past
the three-hour hint window. When this happens, Apache Cassandra comes
with a repair process that can be run to rectify these inconsistencies.

### Note

The repair process does not run on its own, and must be executed
manually. Tools such as Reaper for Apache Cassandra allow for some
automation, including scheduling cluster repairs.

 

Conditions that can cause data inconsistencies essentially do so by
introducing factors that increase the entropy of the stored replicas.
Therefore, Apache Cassandra employs an **anti-entropy** repair
mechanism, which uses a binary Merkle tree to locate inconsistent
replicas across different nodes. Discrepancies are then rectified by
streaming the correct data from another node.

#### Merkle tree calculation

Merkle trees are created from the bottom up, starting with hashes of the
base or leaf data. The leaf data is grouped together by partition
ranges, and then it is hashed with the hash of its neighboring leaf to
create a singular hash for their parent. Parents are hashed together
with neighboring parents (to create their parents), until (Kozliner
2017) all have been combined into a single hash, known as the root.
Based on the hash value of the root for a token range, the Merkle trees
of different nodes can be quickly compared.

### Note

A Merkle tree calculation can consume high amounts of resources (CPU,
disk I/O) on a node, which can result in temporary query timeouts as the
node continues to try to serve requests. For this reason, it is best to
run repairs during times when client requests are low. The Merkle tree
calculation part of the repair processes can be monitored with
`nodetool compactionstats` , where it will display as a
**validation compaction**.

Let’s walk through a quick example. Assume that we've added high scores
for more users, we're running a repair on the `hi_scores` 
table, and we're only concerned about the token range of
`-9223372036854775807` to `-7686143364045646507` .
We'll query our the `hi_scores` table for our users whose
token value for `name` (our partition key) falls within that
range:

Copy

``` {.programlisting .language-markup}
cassdba@cqlsh&gt; SELECT DISTINCT name,token(name) FROM fenago.hi_scores
   WHERE token(name) &gt;= -9223372036854775807
     AND token(name) &lt;= -7686143364045646507;

 name   | system.token(name)
--------+----------------------
 Connor | -8880179871968770066
    Rob | -8366205352999279036
    Dad | -8339008252759389761
   Ryan | -8246129210631849419

(4 rows)
```

### Note

Executing a range query on a partition key is only possible when also
using the `token` function, as shown previously.

Now let's compute a hash (MD5 is used for this example) for all data
within each partition:

*        Connor: h(dCo) = c13eb9350eb2e72eeb172c489faa3d7f*

* Rob: h(dRo) = 97ab517e5418ad2fe700ae45b0ffc5f3*

*     Dad: h(dDa) = c9df57b434ad9a674a242e5c70587d8b*

* Ryan: h(dRy) = 9b961a89e744c86f5bffc1864b166514*

To build a Merkle tree for this data, we start by listing each partition
of hashed data separately, as leaf nodes. Then we will backtrack toward
our root node by applying the hash function,` h()` , while
combining our data recursively:

![](https://raw.githubusercontent.com/fenago/apache-cassandra-intellij/master/md_files/mastering_images/1774c596-cf3e-4ba6-93c3-5b78fe54ebb9.jpeg)

Figure 2.5: An example Merkle tree calculation for a node's token range
containing four partitions of data. This illustrates how they are hashed
and combined to compute the hash of the root (hCRDR).

 

 

Once the data for the partitions is hashed, it is then combined with its
neighbor nodes to generate their parent node:

*hCoRo = h(h(dCo) + h(dRo)) = 3752f8e5673ce8d4f53513aa2cabad40*

*hDaRy = h(h(dDa) + h(dRy)) = 394de4983b242dcd1603eecc35545a6d*

Likewise, the final hash for the root node can be calculated as *Root
node = hCRDR = h(hCoRo + hDaRy) = 7077d9488b37f4d39c908eb7560f2323*.

We can then perform this same Merkle tree calculation on another node
(which is also responsible for the token range of *-9223372036854775807*
to *-7686143364045646507*). If all of the underlying data is consistent
(and affirms our state of anti-entropy), they should all respond with a
root node hash of *7077d9488b37f4d39c908eb7560f2323*. If one or more of
them does not, the up-to-date node then prepares to stream a subset of
the affected range of data to fix the inconsistency.

#### Streaming data

Once discrepancies are identified via Merkle trees, data files are
streamed to the node containing the inconsistent data. Once this
streaming completes, the data should be consistent across all replicas.

### Note

Repair streams consume network resources that can also interfere with a
node's ability to serve requests. Repair progress can be monitored using
the `nodetool netstats` command.

### Read repair

Read repair is a feature that allows for inconsistent data to be fixed
at query time. `read_repair_chance` (and
`dclocal_read_repair_chance` ) is configured for each table,
defaulting to 0.1 (10% chance). When it occurs (only with read
consistency levels higher than `ONE` ), Apache Cassandra will
verify the consistency of the replica with all other nodes not involved
in the read request. If the requested data is found to be inconsistent,
it is fixed immediately.

 

Do note that read repairs do not occur when read consistency is set to
`ONE` or `LOCAL_ONE` . Also, reads at a consistency
level of all will force a read repair to happen 100% of the time.
However, these read repairs do incur latency, which is why they are
typically set to low percentages.

### Note

Sometimes a single partition or key will be all that is inconsistent. If
you find yourself in this situation, forcing a read repair is a simple
task via cqlsh. If the number of inconsistent rows is a manageable
number, you can fix them by setting your consistency level to all, and
querying each of them.

### Security

One criticism of NoSQL data stores is that security is often an
afterthought. Cassandra is no exception to this view. In fact, Apache
Cassandra will install with all security features initially disabled.
While clusters should be built on secured networks behind
enterprise-grade firewalls, by itself this is simply not enough. There
are several features that Cassandra provides that can help to tighten up
security on a cluster.

#### Authentication

Apache Cassandra allows for user authentication to be set up. Similar to
other Cassandra components, users and their (encrypted) credentials are
stored in a table inside the cluster. Client connections are required to
provide valid username/password combinations before being allowed access
to the cluster. To enable this functionality, simply change the
`authenticator` property in the
`cassandra.yaml`  file from its default
(`AllowAllAuthenticator` )
to `PasswordAuthenticator` :

Copy

``` {.programlisting .language-markup}
authenticator: PasswordAuthenticator
```

### Note

Once you have created your own DBA-level superuser account for
administrative purposes, you should never use the default cassandra user
again. This is because the Cassandra user is the only user that has
special provisions in the code to query the `system_auth` 
tables at a consistency level higher than `ONE` (it uses
`QUORUM` ). This means that the default Cassandra user may not
be able to log in if multiple nodes are down.

When user authentication is enabled, Cassandra will create the
`system_auth` keyspace. When
using `PasswordAuthenticator` , `CassandraRoleManager` 
must also be used.

 

 

 

#### Authorization

With authentication set up, additionally enabling user authorization
allows each user to be assigned specific permissions to different
Cassandra objects. In Apache Cassandra versions 2.2 and up, users are
referred to as **roles**. With authorization enabled, roles can be
assigned the following permissions via CQL:

-   `ALTER` : Change table or keyspace definitions
-   `AUTHORIZE` : Grant or revoke specific permissions to other
    users
-   `CREATE:` : Create new tables or keyspaces
-   `DESCRIBE` : Retrieve definitions of a table or keyspace
-   `DROP` : Delete specific tables or keyspaces
-   `EXECUTE` : Run functions in a specific keyspace
-   `MODIFY` : Run `DELETE` , `INSERT` ,
    `UPDATE` , or `TRUNCATE` commands on a specific
    table
-   `SELECT` : Query a specific table
-   `ALL PERMISSIONS` : No restricted access on specific tables
    or keyspaces

The following are some examples:

Copy

``` {.programlisting .language-markup}
GRANT SELECT ON KEYSPACE item TO item_app_readonly;
GRANT MODIFY ON KEYSPACE store TO store_app_user;
GRANT ALL PERMISSIONS ON KEYSPACE mobile TO mobile_admin;
```

#### Managing roles

As role-management is part of the user security backend, it
requires `PasswordAuthenticator` to be enabled before its
functionality is activated. The role system allows you to assign
permissions to a role, and then `GRANT` access to that role
for another user/role. As mentioned in the previous section, role
management is new as of Apache Cassandra 2.2, and can greatly simplify
user-management and permission-assignment.

Here is an example:

Copy

``` {.programlisting .language-markup}
CREATE ROLE cassdba WITH SUPERUSER=true AND LOGIN=true AND PASSWORD='bacon123';
CREATE ROLE supply_chain_rw WITH LOGIN=true AND PASSWORD='avbiuo2t48';
```

 

 

#### Client-to-node SSL

Enabling client-to-node **Secure Socket Layer** (**SSL**) security has
two main benefits. First, each client connecting to the cluster must
present a valid certificate that matches a certificate or **Certificate
Authority** (**CA**) in the node's Java KeyStore. Second, upon
successful connection (and cert validation), all traffic between the
client and node will be encrypted.

#### Node-to-node SSL

Enabling node-to-node SSL security is designed to prevent a specific
avenue of attack. A node will not be allowed to join the cluster, unless
it presents a valid SSL certificate in its Java KeyStore and the Java
TrustStore matches with the other nodes in the cluster. When this level
of security is active, it will encrypt communication between the nodes
over port `7001` .

### Note

Node-to-node SSL may seem unnecessary, but without it, an internal
attacker could join a rogue node to your cluster. Then once it has
bootstrapped, it will shut the node down and its data directories will
contain at least some of your data. If you don't enable node-to-node
SSL, this can be done without having to know any of the admin passwords
to authenticate to the cluster.
