Cassandra's ring architecture
-----------------------------

* * * * *

An aspect of Cassandra's architecture that demonstrates its AP CAP
designation is in how each instance works together. A single-instance
running in Cassandra is known as a **node**. A group of nodes serving
the same dataset is known as a **cluster** or **ring**. Data written is
distributed around the nodes in the cluster. The partition key of the
data is hashed to determine it's **token**. The data is sent to the
nodes responsible for the token ranges that contain the hashed token
value.

### Note

The consistent hashing algorithm is used in many distributed systems,
because it has intrinsic ways of dealing with changing range
assignments.You can refer to *Cassandra High Availability*
by* *Strickland R. (2014), published by fenago.

 

The partition key (formerly known as a **row key**) is the first part of
`PRIMARY KEY` , and the key that determines the row’s token or
placement in the cluster.

Each node is also assigned additional, ancillary token ranges, depending
on the replication factor specified. Therefore, data written at an RF of
three will be written to one primary node, as well as a secondary and
tertiary node.

### Partitioners

The component that helps to determine the nodes responsible for specific
partitions of data is known as the partitioner. Apache Cassandra
installs with three partitioners. You can refer to Apache Cassandra 3.0
for DSE 5.0, *Understanding the Architecture*. Retrieved on 20180404
from:
[https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archTOC.html](https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archTOC.html).
Partitioners differ from each other in both efficiency and
data-distribution strategy.

#### ByteOrderedPartitioner

`ByteOrderedPartitioner` sorts rows in the cluster lexically
by partition key. Queried results are then returned ordered by that key.
Tokens are calculated by the hexadecimal value of the beginning of the
partition key.

When we discuss data modeling and CQL later on, the ability to order a
result set by a partition key may seem like a good idea. But there are
several problems with this partitioner. Foremost among them is that data
distribution typically suffers.

#### RandomPartitioner

`RandomPartitioner` was the default partitioner prior to
Apache Cassandra 1.2. Tokens are calculated using a MD5 hash of the
complete partition key. Possible token values range from zero to
*(2^127^) - 1*. For additional information you can refer to Saha S.
(2017). *The Gossip Protocol - Inside Apache Cassandra*. Retrieved on
20180422 from:
[https://www.linkedin.com/pulse/gossip-protocol-inside-apache-cassandra-soham-saha/](https://www.linkedin.com/pulse/gossip-protocol-inside-apache-cassandra-soham-saha/)

 

#### Murmur3Partitioner

`Murmur3Partitioner` is an improvement in efficiency over
`RandomPartitioner` , and is currently the default partitioner.
The **murmur3** hashing algorithm is more efficient, because Cassandra
does not really need the extra benefits provided by a cryptographic hash
(MD5). Possible token values range from *-(2^63^)* to *(2^63^)-1*.

Important points about Apache Cassandra's partitioners:

-   A partitioner is configured at the cluster level. You cannot
    implement a partitioner at the keyspace or table level.
-   Partitioners are not compatible. Once you select a partitioner, it
    cannot be changed without completely reloading the data.
-   Use of `ByteOrderedPartitioner` is considered to be an
    anti-pattern.
-   `Murmur3Partitioner` is an improvement on the efficiency
    of `RandomPartitioner` .

`Murmur3Partitioner` is the default partitioner, and should be
used in new cluster builds.

### Note

`ByteOrderedPartitioner` and `RandomPartitioner` are
still delivered with Apache Cassandra for backward compatibility,
providing older implementations with the ability to upgrade.

### Single token range per node

Prior to Apache Cassandra 1.2, nodes were assigned contiguous ranges of
token values. With this approach, token ranges had to be computed and
configured manually, by setting the `initial_token` property
in `cassandra.yaml` . A possible token distribution
(using`Murmur3Partitioner` ) with single token ranges assigned
per node is as follows:

  ------------- ---------------------------------- ----------------------------------
  **Node \#**   **Start token**                    **End token**
  0             `5534023222112865485`    `-9223372036854775808` 
  1             `-9223372036854775807`   `-5534023222112865485` 
  2             `-5534023222112865484`   `-1844674407370955162` 
  3             `-1844674407370955161`   `1844674407370955161` 
  4             `1844674407370955162`    `5534023222112865484` 
  ------------- ---------------------------------- ----------------------------------

Table 2.1: An example of the token range assignments for a five-node
cluster, where each node is assigned responsibility for a single,
contiguous token range

With one node responsible for a single, contiguous token range, nodes
would follow each other in the ring. When rows are written to the node
determined by the hashed token value of their partition key, their
remaining replicas are written to ancillary ranges on additional nodes.
The additional nodes designated to hold secondary ranges of data are
determined by walking the ring (or data center, depending on replication
strategy) in a clockwise direction:

![](https://raw.githubusercontent.com/fenago/apache-cassandra-intellij/master/md_files/mastering_images//148a1537-8f7c-40c5-8174-5be36205f7c9.png)

Figure 2.2: A five-node cluster (non-vnodes) illustrating how one node
follows another based on their start and end token ranges

For example, consider the `hi_scores_by_game` table from the
previous lab. As that table uses `game` as its partition
key, data stored in that table will be written to a node based on the
hash of the value it has for `game` . If we were to store data
for the `Pacman` game, its hashed token value would look like
this:

Copy

``` {.programlisting .language-markup}
cassdba@cqlsh:fenago&gt; SELECT game, token(game)
    FROM hi_scores_by_game WHERE game=Pacman;

 game        | system.token(game)
-------------+----------------------
      Pacman | 4538621633640690522
```

The partition key value of `Pacman` hashes to a token of
`4538621633640690522` . Based on the figures depicting the
preceding five-node cluster, node four is primarily responsible for this
data, as the token falls between `1844674407370955162` and
`5534023222112865484` .

Now let's assume that the `fenago`  keyspace has the following
definition:

Copy

``` {.programlisting .language-markup}
CREATE KEYSPACE fenago WITH replication =
  {'class': 'NetworkTopologyStrategy', 'ClockworkAngels': '3'}
  AND durable_writes = true;
```

If all five of the nodes are in the `ClockworkAngels`  data
center, then the nodes responsible for the two additional replicas can
be found by traversing the ring in a clockwise manner. In this case,
node zero is responsible for the second replica of the
`Pacman` high score data, and node one is responsible for the
third replica.

### Note

`NetworkTopologyStrategy` also takes logical rack definition
into account. When determining additional replica placement, this
strategy will attempt to place replicas on different racks, to help
maintain availability in the event of a rack failure scenario.

### Vnodes

Newer versions of Apache Cassandra have an option to use virtual nodes,
better known as **vnodes**. Using vnodes, each node is assigned several
non-contiguous token ranges. This helps the cluster to better balance
its data load:

  ------------- ----------------------------------------------------------------
  **Node \#**   **Token ranges**
  0             `79935 to -92233, -30743 to -18446, 55341 to 67638` 
  1             `-92232 to -79935, -61488 to 61489, 43043 to 55340` 
  2             `-79934 to -67638, -43041 to -30744, 67639 to 79935` 
  3             `-67637 to -55340, 61490 to 18446,18447 to 30744` 
  4             `-55339 to -43042, -18445 to -61489, 30745 to 43042` 
  ------------- ----------------------------------------------------------------

Table 2.2: An example of the token range distribution for a five-node
cluster using vnodes. Each node is assigned responsibility for three,
non-contiguous token ranges. Smaller numbers were used in this example
for brevity.

While the application of virtual nodes may seem more chaotic, it has
several benefits over single, manually-specified token ranges per node:

-   Provides better data distribution over manually-assigned tokens.
-   More numerous, multiple token ranges per node allows multiple nodes
    to participate in bootstrapping and streaming data to a new node.
-   Allows for automatic token range recalculation. This is useful when
    new nodes are added to the cluster, to prevent the existing nodes
    from needing to be reconfigured and bounced (restarted).

### Note

Cassandra 3.0 uses a new token-allocation algorithm that allows the
distribution of data to be optimized for a specific keyspace. Invoking
this algorithm is only possible when using vnodes and
`Murmur3Partitioner` . It requires that a keyspace be specified
for the `allocate_tokens_for_keyspace` property in the
`cassandra.yaml` file. You can refer to Apache Software
Foundation (2016). Documentation: *Adding, replacing, moving and
removing nodes, r*etrieved on 20180418 from:
[http://cassandra.apache.org/doc/latest/operating/topo\_changes.html](http://cassandra.apache.org/doc/latest/operating/topo_changes.html).

